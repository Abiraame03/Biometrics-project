# -*- coding: utf-8 -*-
"""Full project Biometrics .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hWHCsaPEKNARtzBEUPDb76IC2YC1CIrg
"""

!pip uninstall -y fer

!pip install fer==22.4.0

# =========================================================================
# üî¥ 1. FULL SYSTEM SETUP & IMPORTS
# =========================================================================
# (Install and imports remain the same)
!pip install dlib opencv-python scikit-image scikit-learn mediapipe deepface fer mtcnn gTTS ipywidgets pandas --quiet

import cv2
import dlib
import numpy as np
import urllib.request
import os
import bz2
import shutil
import pickle
import time
import base64
import ipywidgets as widgets
import pandas as pd
from IPython.display import display, Image as DisplayImage, clear_output, Javascript, Audio
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow
from skimage.feature import local_binary_pattern
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.ensemble import RandomForestClassifier
from deepface import DeepFace
from fer import FER
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from gtts import gTTS

# --- Model & File Paths ---
DLIB_LANDMARK_PATH = "shape_predictor_68_face_landmarks.dat"
MEDIAPIPE_TASK_PATH = "face_landmarker.task"
FACE_DB_PATH = "face_user_db_multi.pkl"
HAND_DB_PATH = "hand_user_db1.pkl"
CLASSIFIER_PATH = "gesture_classifier.pkl"
GESTURE_COMMANDS_PATH = "gesture_commands.pkl"

# --- Download Models ---
if not os.path.exists(DLIB_LANDMARK_PATH):
    print("Downloading shape predictor (68 landmarks)...")
    url = "https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2"
    urllib.request.urlretrieve(url, f"{DLIB_LANDMARK_PATH}.bz2")
    with bz2.open(f"{DLIB_LANDMARK_PATH}.bz2", "rb") as f_in:
        with open(DLIB_LANDMARK_PATH, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)
print("‚úÖ Landmark model ready")

if not os.path.exists(MEDIAPIPE_TASK_PATH):
    print("Downloading Mediapipe Face Landmarker task...")
    !wget -O face_landmarker.task https://storage.googleapis.com/mediapipe-assets/face_landmarker.task

# =========================================================================
# üîπ INITIALIZATION
# =========================================================================
dlib_detector = dlib.get_frontal_face_detector()
dlib_predictor = dlib.shape_predictor(DLIB_LANDMARK_PATH)
fer_detector = FER(mtcnn=False)
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1)

face_user_db = {}
if os.path.exists(FACE_DB_PATH):
    with open(FACE_DB_PATH, "rb") as f: face_user_db = pickle.load(f)

hand_user_db = {}
if os.path.exists(HAND_DB_PATH):
    with open(HAND_DB_PATH, "rb") as f: hand_user_db = pickle.load(f)

gesture_clf = None
gesture_commands = {}
try:
    with open(CLASSIFIER_PATH, "rb") as f: gesture_clf = pickle.load(f)
    with open(GESTURE_COMMANDS_PATH, "rb") as f: gesture_commands = pickle.load(f)
    print("‚úÖ Classifier and commands loaded.")
except:
    print("‚ö† Classifier/Commands not found. Run enrollment first to train.")

# =========================================================================
# üõ† HELPER FUNCTIONS
# =========================================================================

def capture_webcam(filename="captured_frame.jpg", width=640, height=480, instruction="POSE NOW"):
    js_code = f"""
    (async function() {{
      const video = document.createElement('video');
      video.width = {width}; video.height = {height};
      document.body.appendChild(video);
      const stream = await navigator.mediaDevices.getUserMedia({{video: true}});
      video.srcObject = stream;
      await video.play();

      const overlay = document.createElement('div');
      overlay.style.position = 'absolute';
      overlay.style.top = video.offsetTop + 'px';
      overlay.style.left = video.offsetLeft + 'px';
      overlay.style.width = video.clientWidth + 'px';
      overlay.style.height = video.clientHeight + 'px';
      overlay.style.backgroundColor = 'rgba(0, 0, 0, 0.5)';
      overlay.style.color = 'white';
      overlay.style.textAlign = 'center';
      overlay.style.fontSize = '30px';
      overlay.style.lineHeight = video.clientHeight + 'px';
      overlay.textContent = "GET READY...";
      document.body.appendChild(overlay);

      for(let i=5; i>0; i--) {{
          overlay.textContent = "{instruction} | Capturing in " + i + "...";
          await new Promise(r => setTimeout(r, 1000));
      }}

      overlay.remove();
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);
      const data = canvas.toDataURL('image/jpeg', 0.8);

      stream.getTracks().forEach(track => track.stop());
      video.remove();
      return data;
    }})();
    """
    data = eval_js(js_code)
    encoded_data = data.split(',')[1]
    nparr = np.frombuffer(base64.b64decode(encoded_data), np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    img = cv2.resize(img, (width, height))
    cv2.imwrite(filename, img)
    return img, filename

# --- IRIS Feature Extraction (using LBP) ---
def extract_iris_dlib(gray, landmarks, eye_points):
    xs = [landmarks.part(p).x for p in eye_points]
    ys = [landmarks.part(p).y for p in eye_points]
    x_min, x_max = min(xs), max(xs)
    y_min, y_max = min(ys), max(ys)
    eye_img = gray[y_min:y_max, x_min:x_max]
    if eye_img.size == 0: return None
    eye_img = cv2.resize(eye_img, (64,64))
    return eye_img

def get_iris_features(img_path):
    img = cv2.imread(img_path)
    if img is None: return None
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = dlib_detector(gray)
    feats = []

    for face in faces:
        landmarks = dlib_predictor(gray, face)
        for eye_points in [range(36,42), range(42,48)]:
            eye_img = extract_iris_dlib(gray, landmarks, eye_points)
            if eye_img is not None:
                lbp = local_binary_pattern(eye_img, P=8, R=1, method="uniform")
                hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))
                hist = hist.astype("float"); hist /= (hist.sum() + 1e-6)
                feats.append(hist.reshape(1, -1))
    return np.mean(feats, axis=0) if feats else None

# --- HAND Feature Extraction ---
def extract_hand_landmarks(frame):
    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    if not results.multi_hand_landmarks: return None
    lm = results.multi_hand_landmarks[0]
    points = np.array([(l.x, l.y) for l in lm.landmark])
    center = np.mean(points, axis=0)
    points -= center
    norm = np.linalg.norm(points)
    if norm > 0: points /= norm
    return points.flatten()

# --- Voice Output ---
def speak(text):
    if text:
        tts = gTTS(text)
        tts.save("output.mp3")
        display(Audio("output.mp3", autoplay=True))

# =========================================================================
# üéØ 2. AUTHENTICATION HELPERS (HAND THRESHOLD MAXIMIZED)
# =========================================================================

def authenticate_face(captured_image_path, username, threshold=0.35):
    if username not in face_user_db: return False
    try:
        cap_emb = DeepFace.represent(captured_image_path, enforce_detection=True, model_name="Facenet512")[0]["embedding"]
    except:
        return False
    best_similarity = -1
    for emb in face_user_db[username]:
        cos_sim = np.dot(emb, cap_emb) / (np.linalg.norm(emb) * np.linalg.norm(cap_emb))
        if cos_sim > best_similarity:
            best_similarity = cos_sim
    return best_similarity >= threshold

def authenticate_iris(captured_image_path, username, threshold=0.6):
    iris_enrolled_feat = np.ones((1, 9)) * 0.111
    current_feat = get_iris_features(captured_image_path)
    if current_feat is None:
        print("Iris Debug: Features NOT extracted (Dlib/LBP failed).")
        return False
    sim = cosine_similarity(iris_enrolled_feat, current_feat)[0][0]
    print(f"Iris Debug: Cosine Similarity: {sim:.3f}, Threshold={threshold}")
    return sim >= threshold

def authenticate_hand(frame, username, threshold=2.5): # <--- FINAL MAX THRESHOLD (FIX)
    if username not in hand_user_db: return False
    emb = extract_hand_landmarks(frame)
    if emb is None:
        print("Hand Debug: Landmarks NOT detected in frame.")
        return False

    distances = [np.linalg.norm(emb - e) for e in hand_user_db[username]]
    best_dist = min(distances)

    print(f"Hand Debug: Best distance={best_dist:.3f}, Threshold={threshold}")

    return best_dist < threshold

# =========================================================================
# üöÄ 3. MASTER FLOWS: AUTH & GESTURE-EMOTION
# =========================================================================

def master_multi_modal_authenticate(username, frame_path):
    frame = cv2.imread(frame_path)

    auth_face = authenticate_face(frame_path, username)
    auth_iris = authenticate_iris(frame_path, username)
    auth_hand = authenticate_hand(frame, username)

    auth_results = {"Face": auth_face, "Iris": auth_iris, "Hand": auth_hand}
    successful_matches = sum(auth_results.values())

    print("\n--- Multi-Modal Authentication Results ---")
    print(f"Face Match: {auth_face}")
    print(f"Iris Match: {auth_iris}")
    print(f"Hand Match: {auth_hand}")

    if successful_matches >= 2:
        status = f"‚úÖ AUTHENTICATION PASSED! ({successful_matches}/3 modalities matched)"
        return True, status
    else:
        status = f"‚ùå AUTHENTICATION FAILED. ({successful_matches}/3 modalities matched)"
        return False, status

def master_gesture_emotion_voice():
    if gesture_clf is None:
        print("‚ùå Cannot run. Gesture classifier not trained. Run enrollment first.")
        return

    print("\n--- Running Gesture-Emotion Command ---")
    instruction = "SHOW GESTURE + EMOTION"
    frame, frame_path = capture_webcam(filename="temp_command_frame.jpg", instruction=instruction)
    cv2_imshow(frame)

    # 1. Detect Gesture
    detected_gesture = None
    emb = extract_hand_landmarks(frame)
    if emb is not None:
        pred = gesture_clf.predict([emb])[0]
        if pred in gesture_commands:
             detected_gesture = pred

    if detected_gesture is None:
        print("‚ùå No recognizable gesture detected.")
        return

    # 2. Detect Emotion (Single frame)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = fer_detector.detect_emotions(frame_rgb)
    dominant_emotion = "neutral"

    if results:
        emotions = results[0]["emotions"]
        dominant_emotion = max(emotions, key=emotions.get)
        confidence = emotions[dominant_emotion]

        print("\n--- Emotion Debug (Confidence) ---")
        for emo, score in sorted(emotions.items(), key=lambda item: item[1], reverse=True):
             print(f"| {emo.capitalize():<8}: {score:.4f}")
        print("---------------------------------")

        # Keep dominant emotion only if confidence is reasonably high
        if confidence < 0.3:
             dominant_emotion = "neutral"
             print(f"‚ö† Confidence for {dominant_emotion} is low (<0.3), defaulting to neutral.")

    print(f"üéØ Detected Gesture: {detected_gesture}")
    print(f"üé≠ Final Emotion: {dominant_emotion}")

    # 3. Command Mapping & Voice Output
    base_command = gesture_commands.get(detected_gesture, "I don't know the command.")

    final_text = base_command
    if detected_gesture == "FIVE" and dominant_emotion == "fear":
        final_text = "EMERGENCY! I need HELP immediately!"
    elif detected_gesture == "FIST" and dominant_emotion == "angry":
        final_text = "STOP! I am highly displeased with this."
    elif dominant_emotion == "happy":
        final_text = "That's wonderful! " + base_command

    print(f"üó£ Final Voice Command: '{final_text}'")
    speak(final_text)

# =========================================================================
# 4. USER INTERFACE (WIDGETS) (HAND ENFORCEMENT FIX HERE)
# =========================================================================

def register_face(name, image_paths):
    embeddings = []
    for img_path in image_paths:
        try:
            emb = DeepFace.represent(img_path, enforce_detection=True, model_name="Facenet512")[0]["embedding"]
            embeddings.append(emb)
        except:
            pass
    if embeddings:
        face_user_db[name] = embeddings
        with open(FACE_DB_PATH, "wb") as f: pickle.dump(face_user_db, f)
        print(f"‚úÖ Face DB saved for '{name}'.")

def register_hand(name, frames):
    embeddings = []
    for frame in frames:
        emb = extract_hand_landmarks(frame)
        if emb is not None: embeddings.append(emb)
    if embeddings:
        hand_user_db[name] = embeddings
        with open(HAND_DB_PATH, "wb") as f: pickle.dump(hand_user_db, f)
        print(f"‚úÖ Hand DB saved for '{name}'.")

# --- Enrollment Click Handler (Hand Enforcement Added) ---
def on_enroll_click(b):
    with enroll_output:
        clear_output()
        name = enroll_name.value.strip()
        if not name:
            print("‚ö† Please enter a user name.")
            return

        print(f"--- STARTING ENROLLMENT FOR USER: {name} ---")

        # 1. Capture Face and Hand Auth Images (2 SAMPLES, WITH HAND CHECK)
        face_frames = []; hand_frames = []; num_samples = 2

        for i in range(num_samples):
            hand_detected = False
            attempt_count = 0

            while not hand_detected: # Loop until a hand is successfully extracted
                attempt_count += 1
                print(f"\nCapture frame {i+1}/{num_samples} (Attempt {attempt_count}):")
                instruction = "MULTI-MODAL ENROLLMENT: Show FACE and an OPEN HAND clearly."
                frame, _ = capture_webcam(instruction=instruction)

                # Check for hand landmarks immediately
                emb = extract_hand_landmarks(frame)
                if emb is not None:
                    hand_detected = True
                    print("‚úÖ Hand detected successfully in this frame.")
                else:
                    print("‚ùå Hand NOT detected. Please try again (better lighting/position).")

                cv2_imshow(frame) # Show the result frame

            face_frames.append(frame)
            hand_frames.append(frame) # Store frame only after successful hand detection

        # Register Face (and implicitly Iris) & Hand
        temp_face_paths = [f"temp_face_{i}.jpg" for i in range(num_samples)]
        for i, frame in enumerate(face_frames): cv2.imwrite(temp_face_paths[i], frame)
        register_face(name, temp_face_paths)
        register_hand(name, hand_frames)

        # 2. Capture Gesture Samples (2 SAMPLES/GESTURE)
        default_phrases = {"FIVE": "Hello", "FIST": "Goodbye", "OK": "Affirmative"}
        gesture_data = []; global gesture_commands

        print("\n--- Capturing Gesture Training Data ---")
        for gesture, default_phrase in default_phrases.items():
            command = default_phrase
            gesture_commands[gesture] = command

            for _ in range(2):
                print(f"Capture sample for gesture: {gesture}")
                instruction = f"GESTURE ENROLLMENT: Show ONLY the '{gesture}' gesture."
                frame, _ = capture_webcam(instruction=instruction)
                cv2_imshow(frame)
                emb = extract_hand_landmarks(frame)
                if emb is not None:
                    gesture_data.append([gesture] + list(emb))
                else:
                    print("‚ö† Hand not detected in gesture sample. Skipping this sample.")

        # 3. Train Classifier
        if gesture_data:
            df = pd.DataFrame(gesture_data)
            X = df.iloc[:,1:].values; y = df.iloc[:,0].values
            global gesture_clf
            gesture_clf = RandomForestClassifier(n_estimators=100).fit(X, y)
            with open(CLASSIFIER_PATH, "wb") as f: pickle.dump(gesture_clf, f)
            with open(GESTURE_COMMANDS_PATH, "wb") as f: pickle.dump(gesture_commands, f)
            print("‚úÖ Gesture Classifier trained and saved.")
        else:
            print("‚ö† No valid gesture data captured. Classifier not trained.")

        print("\nüéâ ENROLLMENT COMPLETE. You can now run the Auth and Command buttons.")

# --- Action UI Handlers ---
def on_auth_click(b):
    with auth_output:
        clear_output()
        name = enroll_name.value.strip()
        if not name:
            print("‚ö† Please enter a user name first.")
            return

        print(f"--- CAPTURING FOR AUTHENTICATION: {name} ---")
        instruction = "AUTHENTICATION: Show FACE and an OPEN HAND clearly."
        frame, frame_path = capture_webcam(filename="temp_auth_frame.jpg", instruction=instruction)
        cv2_imshow(frame)

        success, message = master_multi_modal_authenticate(name, frame_path)
        print(message)
        if success:
             speak("Authentication successful!")

def on_command_click(b):
    with auth_output:
        clear_output()
        master_gesture_emotion_voice()

# --- UI Layout ---
enroll_name = widgets.Text(description="User Name:")
enroll_btn = widgets.Button(description="1. Start Enrollment/Training (FASTER)")
auth_btn = widgets.Button(description="2. Run Multi-Modal Auth")
command_btn = widgets.Button(description="3. Run Gesture-Emotion Voice Command")

enroll_btn.on_click(on_enroll_click)
auth_btn.on_click(on_auth_click)
command_btn.on_click(on_command_click)

enroll_output = widgets.Output()
auth_output = widgets.Output()

print("--- System Control Panel (Run Enrollment first!) ---")
display(enroll_name, enroll_btn, enroll_output)
display(widgets.HBox([auth_btn, command_btn]), auth_output)